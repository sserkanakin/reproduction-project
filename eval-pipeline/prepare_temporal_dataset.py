#!/usr/bin/env python3
"""
prepare_temporal_dataset.py
===========================
Convert MMIU *temporal_ordering* items into the multi‑image JSONL conversation
format required by **LLaVA‑Interleave**. Optionally augments each training
sample with a chain‑of‑thought explanation generated by GPT‑4o (or any Chat
model) via the **OpenAI Python ≥ 1.0** client.

Quick usage
-----------
```bash
python prepare_temporal_dataset.py \
       --mmiu_file  eval-pipeline/data/Continuous-temporal/temporal_ordering.json \
       --output_dir eval-pipeline/data/finetune_data \
       --train_size 150 \
       --generate_reasoning         # add CoT using GPT‑4o
```
The script writes `train.jsonl` and `test.jsonl` inside `--output_dir`.
"""
from __future__ import annotations

import argparse
import json
import os
import random
import re
import sys
from pathlib import Path
from typing import Any, Dict, List

import tqdm
from dotenv import load_dotenv

################################################################################
# Helper functions
################################################################################

def parse_options(block: str) -> Dict[str, List[int]]:
    """Return mapping like ``{"A": [4,1,6…], "B": [...]}``."""
    out: Dict[str, List[int]] = {}
    for line in block.strip().splitlines():
        m = re.match(r"([A-Z]):\s*\[(.*)\]", line.strip())
        if m:
            letter, indices = m.groups()
            out[letter] = [int(x.strip()) for x in indices.split(',') if x.strip()]
    if not out:
        raise ValueError(f"Could not parse options block:\n{block}")
    return out


def build_user_prompt(images: List[str], question: str) -> str:
    placeholders = "\n".join(f"<image_{i}>" for i in range(len(images)))
    return f"{placeholders}\n\n{question.strip()}"

################################################################################
# Chain‑of‑thought via OpenAI (≥1.0 client)
################################################################################

def call_openai_reasoning(answer: List[int], model: str) -> str:
    """Return *answer + reasoning*.  Falls back to a placeholder on any failure.
    The placeholder guarantees the assistant turn is never empty, which keeps
    training stable even offline.
    """
    answer_str = ", ".join(map(str, answer))
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return (
            f"The correct order is [{answer_str}].\n\n"
            "**Reasoning (placeholder)**: OPENAI_API_KEY not set."
        )

    try:
        import openai  # >=1.0.0

        client = openai.OpenAI(api_key=api_key)
        completion = client.chat.completions.create(
            model=model,
            temperature=0.4,
            max_tokens=300,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert vision‑language tutor. In 3‑6 numbered "
                        "steps, explain why the given ground‑truth temporal order of "
                        "images is correct. Reference image indices 0‑6 when needed."
                    ),
                },
                {
                    "role": "user",
                    "content": f"Ground‑truth order: {answer}. Provide the reasoning.",
                },
            ],
        )
        reasoning = completion.choices[0].message.content.strip()
    except Exception as exc:  # noqa: BLE001
        reasoning = f"**Reasoning (placeholder)**: OpenAI call failed – {exc}."

    return f"The correct order is [{answer_str}].\n\n{reasoning}"

################################################################################
# Conversion logic
################################################################################

def convert_sample(raw: Dict[str, Any], *, add_cot: bool, model: str) -> Dict[str, Any]:
    imgs: List[str] = raw["input_image_path"]
    prompt = build_user_prompt(imgs, raw["question"])

    gt_letter = raw["output"].strip()
    gt_order = parse_options(raw["options"])[gt_letter]

    assistant = (
        call_openai_reasoning(gt_order, model=model) if add_cot else f"The correct order is {gt_order}."
    )

    return {
        "id": raw.get("id", os.path.splitext(os.path.basename(imgs[0]))[0]),
        "images": imgs,
        "conversations": [
            {"from": "human", "value": prompt},
            {"from": "gpt", "value": assistant},
        ],
    }

################################################################################
# Main CLI
################################################################################

def main() -> None:
    load_dotenv()

    ap = argparse.ArgumentParser(description="Prepare MMIU temporal‑ordering for LLaVA‑Interleave")
    ap.add_argument("--mmiu_file", type=Path, required=True, help="Raw MMIU JSON(.l) file")
    ap.add_argument("--output_dir", type=Path, required=True, help="Folder for train/test JSONL")
    ap.add_argument("--train_size", type=int, default=150, help="Samples for training split")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--generate_reasoning", action="store_true", help="Add chain‑of‑thought")
    ap.add_argument("--reasoning_model", default="gpt-4o-mini")
    args = ap.parse_args()

    random.seed(args.seed)
    args.output_dir.mkdir(parents=True, exist_ok=True)

    # ------- load raw -------
    with args.mmiu_file.open(encoding="utf-8") as f:
        raw_samples = json.load(f) if args.mmiu_file.suffix == ".json" else [json.loads(line) for line in f]

    if args.train_size > len(raw_samples):
        sys.exit("train_size exceeds dataset size")

    random.shuffle(raw_samples)
    train_raw = raw_samples[: args.train_size]
    test_raw = raw_samples[args.train_size:]

    # ------- convert & write -------
    for split_name, raw_subset, add_cot in (
        ("train", train_raw, args.generate_reasoning),
        ("test", test_raw, False),
    ):
        out_path = args.output_dir / f"{split_name}.jsonl"
        with out_path.open("w", encoding="utf-8") as out_f:
            for sample in tqdm.tqdm(raw_subset, desc=split_name):
                obj = convert_sample(sample, add_cot=add_cot, model=args.reasoning_model)
                out_f.write(json.dumps(obj, ensure_ascii=False) + "\n")
        print(f"Wrote {len(raw_subset):>3} {split_name} samples → {out_path}")

if __name__ == "__main__":
    main()
